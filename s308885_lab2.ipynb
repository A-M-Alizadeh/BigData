{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9fd2c76-49c5-46e1-8bac-f64e288186ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputPath = \"/data/students/bigdata_internet/lab2/word_frequency.tsv\"\n",
    "outputPath = \"Lab2_Results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63b623bd-6dc4-4d10-bebf-dd5d6ac07158",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "339819"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.textFile(inputPath)\n",
    "new_rdd = rdd.map(lambda x: x.split('\\t'))\n",
    "final_rdd = new_rdd.map(lambda x: (x[0], int(x[1])))\n",
    "final_rdd.take(5)\n",
    "final_rdd.top(5,lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d001f8df-929a-4748-a2e2-bbf36ebefa79",
   "metadata": {},
   "source": [
    "- reading the file from that path\n",
    "- splitting the each item using '\\t'\n",
    "- csting the second parth which the frequency to integer\n",
    "- taking 5 samples from rdd\n",
    "- picking the first most frequest words\n",
    "\n",
    "there are 339819 words in total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaacda70-ef6d-47ef-a816-e8a3eb65d140",
   "metadata": {},
   "source": [
    "### 1.0.1\n",
    "drawing 5 samples from RDD using take() function\n",
    "\n",
    "### 1.0.2\n",
    "for picking the top nth row we would use top() function which the second parameter is the function for sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b773423f-79fb-4556-b58b-657e218e8205",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "339819"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de60df7b-3773-40ba-b74c-f91ceb4425b4",
   "metadata": {},
   "source": [
    "### 1.0.3\n",
    "- using the count method on rdd we get the total number of words which is 339819\n",
    "- if there are repeatitive words we can use distinct() function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d12cb8d-5c4b-4ae0-aab7-884fe1e63be3",
   "metadata": {},
   "source": [
    "### 1.0.4\n",
    "it is a file. A TSV file is a tab-separated values file commonly used by spreadsheet applications to exchange data between databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "884a49a7-201d-4ea3-948f-2d2a3082d26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'ho'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5091bbc1-aa7d-40ae-af1b-f8164d4cfd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('how', 36264)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fltr_rdd = final_rdd.filter(lambda x : x[0].startswith(prefix))\n",
    "fltr_rdd.count()\n",
    "fltr_rdd.top(1, lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdbd902-cdd4-411c-8985-16734bc6e237",
   "metadata": {},
   "source": [
    "### 1.1.1\n",
    "there are 1519 words which start with 'ho' as prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39eedcb-8d2c-4eda-b9d5-d68adb25dbff",
   "metadata": {},
   "source": [
    "### 1.1.2\n",
    "top most frequest word starting with 'ho' is [('how', 36264)]\n",
    "\n",
    "- first we filter the original RDD to get the words starting with 'ho'\n",
    "- use top() method to get tghe most frequent then get the first item of the rdd then get the second field of the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adb15c67-2b78-4c73-bd58-bfe409491513",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:=============================>                             (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max frequent:  36264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fltr_rdd = final_rdd.filter(lambda x : x[0].startswith(prefix))\n",
    "print('max frequent: ', fltr_rdd.top(1, lambda x: x[1])[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49960354-3b8a-4a09-8f8e-9d447558649b",
   "metadata": {},
   "source": [
    "### 1.1.2\n",
    "second method\n",
    "\n",
    "- first we filter the original RDD to get the words starting with 'ho'\n",
    "- sort the resulted rdd using takeordered() function which the second part is the method for sorting\n",
    "- printing the second filed of the array of the first row of the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e3332d4-84e3-44da-87f5-57c8ad742903",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sorted_rdd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_49/2667984825.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfltr_rdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmax_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfltr_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtakeOrdered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted_rdd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sorted_rdd' is not defined"
     ]
    }
   ],
   "source": [
    "fltr_rdd = final_rdd.filter(lambda x : x[0].startswith(prefix))\n",
    "max_freq = fltr_rdd.takeOrdered(1, lambda n: -1*n[1])\n",
    "print(sorted_rdd[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d6b820-8202-4805-b43b-95086c727cc0",
   "metadata": {},
   "source": [
    "### 1.1.2\n",
    "third method\n",
    "\n",
    "- first we filter the original RDD to get the words starting with 'ho'\n",
    "- sorting the rdd using sortBy() method according to the second field of array\n",
    "- printing the second field of the first array of the first resulted rdd picked by take()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f775e4df-b234-4d0a-8aba-60bacc88a847",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:=============================>                             (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fltr_rdd = final_rdd.filter(lambda x : x[0].startswith(prefix))\n",
    "sortedPairs = fltr_rdd.sortBy(lambda pair: pair[1], False)\n",
    "max_freq = (sortedPairs.take(1))[0][1]\n",
    "print(max_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc3c6c0-4ef3-487d-8614-128e1f9d0ba5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.3\n",
    "## filtering the top 70% most frequent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3924edf9-0e2c-443d-be1f-7e3e9cf6618a",
   "metadata": {},
   "source": [
    "### 1.3.1\n",
    "\n",
    "counting the number of the words that their frequency is >= 70% of the max_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17523a71-81f1-4543-bc45-3cdd9c8d49a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:=============================>                            (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "max_70freq_rdd = fltr_rdd.filter(lambda el: el[1] > max_freq*0.7)\n",
    "max70count = max_70freq_rdd.count()\n",
    "print(max70count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baddd431-4dd2-4725-a7c5-5d5835f56e41",
   "metadata": {},
   "source": [
    "1.3.2\n",
    "\n",
    "saving the words the hdfs\n",
    "\n",
    "- mapping each line of the rdd to change it to the desired style word;\n",
    "- path to save in HDFS\n",
    "- saving the resulted RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d08c9268-ceb6-4651-9cc6-8c8765bd024f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_words = max_70freq_rdd.map(lambda x: x[0]+';')\n",
    "max_filtered_path = 'lab2_70_max_freq_words/'\n",
    "# final_words.collect()\n",
    "final_words.saveAsTextFile(max_filtered_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7c509f-29bc-45cd-8056-a24d49e0e9a3",
   "metadata": {},
   "source": [
    "# 2\n",
    "## python script\n",
    "### script used in the file is as shown:\n",
    "- almost every line is as the same as before but with a small change in the last line\n",
    "- mapping all each line and changing the style to the desired one word - frequency,\n",
    "\n",
    "<code>\n",
    "import os\n",
    "import sys\n",
    "from pyspark import SparkConf, SparkContext\n",
    "    \n",
    "conf = SparkConf().setAppName(\"My app\")\n",
    "sc = SparkContext(conf = conf)\n",
    "inputPath = \"/data/students/bigdata_internet/lab2/word_frequency.tsv\"\n",
    "outputFolderName = str(sys.argv[1]) + '/'\n",
    "prefixStr = str(sys.argv[2])\n",
    "rdd = sc.textFile(inputPath)\n",
    "new_rdd = rdd.map(lambda x: x.split('\\t'))\n",
    "final_rdd = new_rdd.map(lambda x: (x[0], int(x[1])))\n",
    "fltr_rdd = final_rdd.filter(lambda x : x[0].startswith(prefixStr))\n",
    "changed_rdd = fltr_rdd.map(lambda x: (x[0] + ' - ' + str(x[1]) +','))\n",
    "changed_rdd.saveAsTextFile(outputFolderName)\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3698cec6-73d8-4de9-a5c3-397b11f42769",
   "metadata": {},
   "source": [
    "### 2.1\n",
    "spark-submit --master local --deploy-mode client 'lab2_prefix_script.py' lab2scriptfolder ho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d778b3dd-671a-4177-9f5b-c5e559bc1af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: User-defined SPARK_HOME (/opt/cloudera/parcels/CDH-6.2.1-1.cdh6.2.1.p0.1425774/lib/spark) overrides detected (/opt/cloudera/parcels/CDH/lib/spark).\n",
      "WARNING: Running spark-class from user-defined location.\n",
      "22/12/20 00:04:20 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/12/20 00:04:20 WARN util.Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "Program took 4.645367622375488 seconds to run                                   \n"
     ]
    }
   ],
   "source": [
    "!spark-submit --master local --deploy-mode client 'lab2_prefix_script.py' lab22scriptt ho"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa1f8a8-591b-4a9f-afcf-11ef3810091a",
   "metadata": {},
   "source": [
    "the command shown above execution time is :\n",
    "- Program took 4.714531421661377 seconds to run\n",
    "- Program took 4.645367622375488 seconds to run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2d6342c5-c1c3-4d6b-acd9-bd1bb02da7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: User-defined SPARK_HOME (/opt/cloudera/parcels/CDH-6.2.1-1.cdh6.2.1.p0.1425774/lib/spark) overrides detected (/opt/cloudera/parcels/CDH/lib/spark).\n",
      "WARNING: Running spark-class from user-defined location.\n",
      "22/12/20 00:07:13 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/12/20 00:07:13 WARN util.Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/12/20 00:07:22 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/students/s308885/lab2_prefix_script.py\", line 22, in <module>\n",
      "    changed_rdd.saveAsTextFile(outputFolderName)\n",
      "  File \"/opt/cloudera/parcels/CDH-6.2.1-1.cdh6.2.1.p0.1425774/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 1570, in saveAsTextFile\n",
      "  File \"/opt/cloudera/parcels/CDH-6.2.1-1.cdh6.2.1.p0.1425774/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "  File \"/opt/cloudera/parcels/CDH-6.2.1-1.cdh6.2.1.p0.1425774/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o64.saveAsTextFile.\n",
      ": org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://BigDataHA/user/s308885/lab2scriptttt already exists\n",
      "\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:287)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1094)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1067)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1032)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:958)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:957)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1499)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1478)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1478)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
      "\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1478)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:550)\n",
      "\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!spark-submit --master yarn --deploy-mode client 'lab2_prefix_script.py' lab2scriptttt ho"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812ac1e5-4f23-41da-8ce8-ec58bdada28e",
   "metadata": {},
   "source": [
    "the commad above was ran couple of times and there was different durations:\n",
    "- Program took 24.257280588150024 seconds to run\n",
    "- Program took 25.14168953895569 seconds to run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749eb10a-fef2-44de-ae88-8a23d33eed1b",
   "metadata": {},
   "source": [
    "# Bonus Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fad25edf-9d97-4156-89f0-98bd6edc6479",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "51967093"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "originPath = '/data/students/bigdata_internet/lab2/finefoods_text.txt'\n",
    "\n",
    "def splittor(reading):\n",
    "    fields = reading.split(\" \")\n",
    "    return fields\n",
    "\n",
    "\n",
    "origin_rdd = sc.textFile(originPath)\n",
    "seperated_rdd = origin_rdd.flatMap(splittor)\n",
    "seperated_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce3f742-1e44-484f-baf4-6cc35661422a",
   "metadata": {},
   "source": [
    "### 3.1\n",
    "#### total number of words with repeatition is : 51967093\n",
    "\n",
    "- address for the original data\n",
    "- function for the flatmap() method in order to seperate words of a string of each line\n",
    "- reading the file\n",
    "- using the flatMap() to get a single RDD of the words of eachline, if we use map then it would give us arrays of words of each line\n",
    "- counting the words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5680025f-a19d-4f98-8028-df65eb093a3d",
   "metadata": {},
   "source": [
    "## 3.2 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932cfc74-6329-49e7-84d2-6557b87f6a1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a56238-c050-406b-905d-0233b0380770",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fe1acf-b8a1-474b-a25c-39e7a0149e35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c7cb24-53b8-4fc0-9ea2-32ed95ad8356",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Yarn)",
   "language": "python",
   "name": "pyspark_yarn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52dbd3f1-a1af-4561-91cd-81f15efbc9a2",
   "metadata": {},
   "source": [
    "# Lab 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66d968c9-eaf0-4dd8-bdde-3ca408978a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "registerPath = \"/data/students/bigdata_internet/lab3/register.csv\"\n",
    "statiosPath = \"/data/students/bigdata_internet/lab3/stations.csv\"\n",
    "outputPath = \"s308885_Lab3_Solution/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d12e7d8c-c3ce-42f1-92e7-bc4c5201a402",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the files from the paths\n",
    "registerRdd = sc.textFile(registerPath)\n",
    "stationRdd = sc.textFile(statiosPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "043fbcb5-eee4-4cc4-9ec8-75ab3e1740f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#dividing data with \\t\n",
    "#removing the first row which is the header and we don't need it\n",
    "newRegRdd = registerRdd.map(lambda x: x.split('\\t'))\n",
    "Regـheader = newRegRdd.first()\n",
    "noHeaderRegRdd = newRegRdd.filter(lambda x: x!= Regـheader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1f05c1e-2f3a-45bb-a074-4cb3482e759d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25319028"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filtering the data. since there are wrong data that free_slots and used_slots both are 0\n",
    "# counting the number of data after cleaning\n",
    "filteredRegRdd = noHeaderRegRdd.filter(lambda x: x[2]!=\"0\" or x[3]!=\"0\")\n",
    "noHeaderRegRdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e153de-ba89-4634-88f8-115301551d57",
   "metadata": {},
   "source": [
    "## 1.1.1\n",
    "- number of remaining rows after filtering was 25319028 withought header\n",
    "- number of remaining rows after filtering is 25104121 withought header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6889b64-fbd7-46b1-87f6-05c3625e8486",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting station data\n",
    "#selecting the header\n",
    "#removing the header\n",
    "newStatRdd = stationRdd.map(lambda x: x.split('\\t'))\n",
    "Stat_header = newStatRdd.first()\n",
    "noHeaderStatRdd = newStatRdd.filter(lambda x: x!= Stat_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76627056-212e-4b54-826f-f320f8b51b86",
   "metadata": {},
   "source": [
    "# 2\n",
    "\n",
    "- changing the style from existing date to (id, (day, hour), used solts, free slots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21d42d6b-3fa2-40ab-97d8-472b82fe61b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing the timestamp to weekday and hour\n",
    "# the data structure now is (station, weekday, hour, used_slots, free_slots)\n",
    "slottedRdd = filteredRegRdd.map(lambda x: (x[0],(datetime.strftime(datetime.strptime(x[1], \"%Y-%m-%d %H:%M:%S\"),\"%A %H\").split(\" \")[0], datetime.strftime(datetime.strptime(x[1], \"%Y-%m-%d %H:%M:%S\"),\"%A %H\").split(\" \")[1]),x[2],x[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b106b8fa-29e1-4071-be6b-2c3fb0bdc5f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[7] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#caching the data to be able to use it faster later\n",
    "slottedRdd.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3567e19d-211c-495e-8092-d4e8f8788407",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25104121"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#counting the data after cleaning\n",
    "#result is 25104121\n",
    "slottedRdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa9fd33-7b69-47c6-a0cf-8435b2bf3314",
   "metadata": {},
   "source": [
    "#### now we get the rdd grouped by the hours and the result would have 168 = 7*24 row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b716ecf-76ac-45ba-a977-0e53f377fae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grouping the data according to station_id and weekday to have all the rows related to same id , weekday and hour in the same group\n",
    "#the structure is           [(('234', ('Friday', '17')),<pyspark.resultiterable.ResultIterable at 0x7f5967dcbd50>)]\n",
    "grouped_rdd = slottedRdd.groupBy(lambda x: (x[0],x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e9c8b60-3adb-4be8-a6cc-0456c37e2c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the second part after grouping is not readable so we change it to a list\n",
    "listed_result_rdd = grouped_rdd.map(lambda x: (x[0], list(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cf486ae-7658-4b4e-a4e5-d9b1f1ef6368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to conut the number of zeros in each group to calculate the criticality\n",
    "def iterrr(items):\n",
    "    zeros = 0;\n",
    "    for item in items:\n",
    "        if item[3] == '0':\n",
    "            zeros+=1\n",
    "    return zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5fc5127-8f85-448f-8d21-876f8f2a44b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we add a column which is length of the array in the second index of each group\n",
    "# to have number of the rows in each group\n",
    "new_res = listed_result_rdd.map(lambda x: (x[0],x[1],len(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7abebf93-3e1f-4b4e-8e73-3845e2c6360c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we call the function to get the zeros of each group\n",
    "result_with_zeros = new_res.map(lambda x: ( x[0], x[1], x[2], iterrr(x[1]) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b9ad404-939d-4b30-9eeb-3d74b1f79554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that we have total rows and number of zeros in each group we can calculate the criticality\n",
    "critical_ratio_rdd = result_with_zeros.map(lambda x: ( x[0], x[1], x[2], x[3], x[3]/x[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce7f0900-b898-43b1-840e-2f9aef0d372f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##import sys\n",
    "# min_threshold = sys.argv[0]\n",
    "# if we want to pass the threshold we can use the argument which can be passed through terminal commands\n",
    "min_threshold = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7f0956b-8a07-448e-b85b-a847d45f3ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have a minimum threshold which is 0.6. now we filter the data (each group which has criticality rate over min threshold)\n",
    "filtered_critical_rdd = critical_ratio_rdd.filter(lambda x: x[4]>min_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97080df6-d3e9-4f5b-ac88-08be67509159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we want to sort by somw of the columns but problem is weekdays can't be sorted properly since they are strings\n",
    "# so we make a key-value pair which we can use\n",
    "weekdays = { 'Monday': 1, 'Tuesday': 2, 'Wednesday': 3, 'Thursday': 4, 'Friday': 5,'Saturday': 6, 'Sunday': 7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "926b1bd8-27fe-4975-a19f-d3d1c3e0295b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# data is sorted by these items and priority is as follows\n",
    "# first criticality_rate, station_id, weekday, hour\n",
    "#for weekday we use the object we built in the previous step weekdays.get(\"key\") => value which is a number and can be sorted\n",
    "sorted_rdd = filtered_critical_rdd.sortBy(lambda x: (x[4], x[0][0], weekdays.get(x[0][1][0]),x[0][1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec608321-c988-4a74-a1f6-c25bd1496f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we have all the data we needed now we change the structure for further joins\n",
    "#we change it to (key, value) whcich looks like this ('9', ('Friday', '10', 0.6129032258064516))\n",
    "#then we can join two tables by their ids\n",
    "noLattitude_rdd = sorted_rdd.map(lambda x: (x[0][0],(x[1][0][1][0],x[1][0][1][1], x[4])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fdf722a0-6c34-41cd-bb67-155b85b9edcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we need to change the structure of the station rdd to (id,(lat, long)) to make the join possible\n",
    "station_rdd_pair = noHeaderStatRdd.map(lambda x : (x[0],(x[1],x[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1504e52d-b637-4308-a4c5-09c679c2035e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we joing two tables which is joined based on the key\n",
    "#each row looks like this ('10', (('Saturday', '00', 0.622107969151671), ('2.185206', '41.384875')))\n",
    "joined_rdd = noLattitude_rdd.join(station_rdd_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a33494e6-4374-4321-9057-0a9474aa41eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we change the structure the way we need. so we take out the elements of each row according to example\n",
    "#and each row looks like this ('10', '2.185206', '41.384875', 'Saturday', '00', 0.622107969151671)\n",
    "final_joined_rdd = joined_rdd.map(lambda x: (x[0],x[1][1][0],x[1][1][1],x[1][0][0],x[1][0][1], x[1][0][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "43cef039-45b3-493f-8ce3-3674f20076ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('10', '2.185206', '41.384875', 'Saturday', '00', 0.622107969151671),\n",
       " ('9', '2.185294', '41.385006', 'Friday', '22', 0.6258389261744967),\n",
       " ('9', '2.185294', '41.385006', 'Friday', '10', 0.6129032258064516),\n",
       " ('58', '2.170736', '41.377536', 'Monday', '00', 0.6323119777158774),\n",
       " ('58', '2.170736', '41.377536', 'Monday', '01', 0.6239554317548747)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_joined_rdd.take(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "878ec1db-d61c-4ccc-b0bc-c8b2254184c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to add the header we create an rdd with the column names\n",
    "#obviously there are different way to do that\n",
    "header = (\"Station\", \"Lattitude\", \"Longtitude\", \"Weekday\", \"Hour\", \"Criticality\")\n",
    "header_rdd = sc.parallelize([header])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "49491889-c9c8-4ac7-94e5-1fb42c66b214",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we make a union to mix two rdds\n",
    "result_rdd = header_rdd.union(final_joined_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fc686503-a205-419a-b199-3d44e4746df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cast data to string then add \\t as seperator\n",
    "saveable_rdd = result_rdd.map(lambda x: str(x[0])+'\\t'+str(x[1])+'\\t'+str(x[2])+'\\t'+str(x[3])+'\\t'+str(x[4])+'\\t'+str(x[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8a92a71c-99bb-4576-bd12-0940726dae4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#saving  the final result as csv file\n",
    "saveable_rdd.coalesce(1).saveAsTextFile(\"lab3_s308885_final_result.csv/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748904b6-cd46-4e83-ac04-3d53edc7657c",
   "metadata": {},
   "source": [
    "# the final result is this\n",
    "[\n",
    "\n",
    "     ('10', '2.185206', '41.384875', 'Saturday', '00', 0.622107969151671),\n",
    "\n",
    "     ('9', '2.185294', '41.385006', 'Friday', '22', 0.6258389261744967),\n",
    "     \n",
    "     ('9', '2.185294', '41.385006', 'Friday', '10', 0.6129032258064516),\n",
    " \n",
    "     ('58', '2.170736', '41.377536', 'Monday', '00', 0.6323119777158774),\n",
    " \n",
    "     ('58', '2.170736', '41.377536', 'Monday', '01', 0.6239554317548747)\n",
    " \n",
    " ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4d3f7f-39c4-481b-a33d-a72dd5bcbcdd",
   "metadata": {},
   "source": [
    "#-----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7023fb1f-8ff8-4a79-8696-065ececc8065",
   "metadata": {},
   "source": [
    "### USING Spark SQL\n",
    "### -------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a5d5a03c-6df0-4fbb-81cc-7ae379df50f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the paths\n",
    "from pyspark.sql.functions import *\n",
    "registerPath = \"/data/students/bigdata_internet/lab3/register.csv\"\n",
    "statiosPath = \"/data/students/bigdata_internet/lab3/stations.csv\"\n",
    "outputPath = \"s308885_Lab3_Solution/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fa8d498e-48f3-4215-b1c0-5be7a5cbce59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# defining how to read the date with delimeter\n",
    "registerDF = spark.read.csv(registerPath, header=True, inferSchema=True, sep= '\\t')\n",
    "stationDF = spark.read.csv(statiosPath, header=True, inferSchema=True, sep= '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "71629b58-928c-4f8d-bafb-eb9907adcbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - `hour(timestamp)` to obtain the hour\n",
    "# - `date_format(timestamp,'EEEE')` to obtain the day of the week\n",
    "# chaging the format of timestamp into two columns of weekday and hour\n",
    "#removing the timestamp column. we don't need it\n",
    "from datetime import datetime\n",
    "registerDF = registerDF.withColumn(\"weekday\", date_format(registerDF.timestamp, 'EEEE'))\n",
    "registerDF = registerDF.withColumn(\"hour\", hour(registerDF.timestamp))\n",
    "registerDF = registerDF.drop(\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4e7fcddc-3d0a-41ce-8400-a01f728ac326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering the data to remove row with the wrong data which is descried in the document\n",
    "filtered_df = registerDF.filter(\"used_slots != 0 or free_slots != 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7ca3bd0d-0b2c-442f-9649-72d704fd23d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining two table name for two dataframes to use in SQL queries\n",
    "filtered_df.createOrReplaceTempView(\"register\")\n",
    "stationDF.createOrReplaceTempView(\"station\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3dff0292-7b5a-4070-bd54-5b89ea14c6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25104121"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#total of row ramaining after filtering is 25104121\n",
    "filtered_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "cfddfe19-7b4d-4392-93cd-ea3a61e5ab25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- station: integer (nullable = true)\n",
      " |-- used_slots: integer (nullable = true)\n",
      " |-- free_slots: integer (nullable = true)\n",
      " |-- weekday: string (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# the schema related to data frame is as given\n",
    "filtered_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a0cc8f36-5a29-42f5-9a0d-aaaf01b03269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the function from pyspark sql module\n",
    "# creating a column for zero values in free slots. if free_slots is zero then the value is 1 otherwise will be 0.\n",
    "# we do this to count zeros after grouping the data\n",
    "# we name the new table zeroRegister\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "withZeros_df = filtered_df.withColumn(\"zeros\", F.when(filtered_df[\"free_slots\"] == 0, 1).otherwise(0))\n",
    "withZeros_df.createOrReplaceTempView(\"zeroRegister\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "111c53ec-1ad0-46f9-bf63-f245f575e6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 153:======================================>                  (4 + 2) / 6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----+--------------------+\n",
      "|station|  weekday|hour|         criticality|\n",
      "+-------+---------+----+--------------------+\n",
      "|    193|Wednesday|  16|                 0.0|\n",
      "|    194|   Friday|   6|                 0.0|\n",
      "|    197|Wednesday|  23|                 0.0|\n",
      "|    197| Saturday|   0|                 0.0|\n",
      "|    201|  Tuesday|  14|                 0.0|\n",
      "|    201|Wednesday|  10|0.001964636542239...|\n",
      "|    202|   Sunday|  15|                 0.0|\n",
      "|    202|   Monday|  13|                 0.0|\n",
      "|    203|   Friday|   6| 0.01011804384485666|\n",
      "|    204|  Tuesday|  19| 0.05016722408026756|\n",
      "|    205| Saturday|  19|                 0.0|\n",
      "|    205|   Sunday|   9|                 0.0|\n",
      "|    205|   Monday|   7| 0.03302752293577982|\n",
      "|    206|   Sunday|  13|0.012302284710017574|\n",
      "|    208|  Tuesday|  15|                 0.1|\n",
      "|    211|   Monday|   7|                 0.0|\n",
      "|    212|   Friday|  23|                 0.0|\n",
      "|    213|   Friday|  18|                0.05|\n",
      "|    213| Thursday|   1| 0.07957559681697612|\n",
      "|    218|Wednesday|  13|                 0.0|\n",
      "+-------+---------+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# now table is group by \"station\", \"weekday\", \"hour\"\n",
    "# two columns are added. first one is total number of zeros\n",
    "# second one is total number of rows for each group\n",
    "# columns are called sum_zeros and count_free_slots\n",
    "# finally we keep the criticality rate caculated using zeros and total which is named criticality\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "grouped_criticality = withZeros_df.groupBy([\"station\", \"weekday\", \"hour\"]) \\\n",
    "       .agg(F.sum(\"zeros\").alias(\"sum_zeros\"), F.count(\"free_slots\").alias(\"count_free_slots\")) \\\n",
    "       .selectExpr(\"station\", \"weekday\", \"hour\", \"sum_zeros / count_free_slots as criticality\")\n",
    "grouped_criticality.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3233dd66-4c6f-4737-b079-a0ddbd4a2ed7",
   "metadata": {},
   "source": [
    "#table looks like this\n",
    "\n",
    "+-------+---------+----+-------------------+\n",
    "\n",
    "|station|  weekday|hour|        criticality|\n",
    "\n",
    "+-------+---------+----+-------------------+\n",
    "\n",
    "|    263|Wednesday|  23|0.16111111111111112|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b5f059d4-5e34-44a3-9e0f-f707942d0ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#minimum criticality is 0.6\n",
    "MinimumCriticality = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "dbc94a1e-a539-40af-bc96-60035e27a0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we filter the groups which have criticality over the min threshold\n",
    "threshold_df = grouped_criticality.select(\"*\").where(grouped_criticality[\"criticality\"] >= 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b3a9c34f-e90d-4fa9-b97a-bbb76a383cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# two tables are joined with a condition threshold_df.station == stationDF.id\n",
    "# then we select the rows we need. join bring all the columns which we don't need\n",
    "joined_dfs = threshold_df.join(stationDF, threshold_df.station == stationDF.id).select(threshold_df.station, stationDF.longitude, stationDF.latitude, threshold_df.weekday, threshold_df.hour, threshold_df.criticality)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bff91b-6985-4022-aec4-6f58dba44a21",
   "metadata": {},
   "source": [
    "#now joined tables look like this\n",
    "+-------+---------+---------+-------+----+------------------+\n",
    "|station|longitude| latitude|weekday|hour|       criticality|\n",
    "+-------+---------+---------+-------+----+------------------+\n",
    "|     58| 2.170736|41.377536| Sunday|  23|0.6239554317548747|\n",
    "|     10| 2.185206|41.384875| Friday|  22| 0.622107969151671|\n",
    "|      9| 2.185294|41.385006| Friday|   8|0.6129032258064516|\n",
    "|     58| 2.170736|41.377536| Sunday|  22|0.6323119777158774|\n",
    "|      9| 2.185294|41.385006| Friday|  20|0.6258389261744967|\n",
    "+-------+---------+---------+-------+----+------------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "adfc91a7-91ac-460d-8751-7271c95c45dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the modules we need\n",
    "# user-defined-functions\n",
    "# integer type to define the new column schema type\n",
    "# importing everything to be sure :)\n",
    "\n",
    "# an object is defined to be used for sorting weekdays and the we create a UDF for that named weekday_order using weekdays.get(x)\n",
    "# weekday_order is a function which returns an specific integer for each day\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql import *\n",
    "\n",
    "weekdays = { 'Monday': 1, 'Tuesday': 2, 'Wednesday': 3, 'Thursday': 4, 'Friday': 5,'Saturday': 6, 'Sunday': 7}\n",
    "# spark.udf.register(\"weekday_order\", lambda x: weekdays.get(x), IntegerType()) //once worked then didn't\n",
    "weekday_order = udf(lambda x: weekdays.get(x), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "54b5fa94-b5d7-4f32-935f-fbe594ecacc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a column named weekday_id and the data is generated using weekday_order UDF\n",
    "\n",
    "joined_dfs = joined_dfs.withColumn(\"weekday_id\", weekday_order(joined_dfs.weekday))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "feab2373-ee83-4a47-8c87-ae06a1468fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have all the data needed we can sort by \"criticality\",\"station\",\"weekday_id\",\"hour\" which are all ascending so the second part is all True\n",
    "sorted_df = joined_dfs.sort(\"criticality\",\"station\",\"weekday_id\",\"hour\",ascending=[True,True,True,True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "91cd4846-166c-4269-82ca-1f35ac30a78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weekday_id column is not needed so we drop it\n",
    "final_DF = sorted_df.drop(\"weekday_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0637b979-f516-4db8-91bd-87b5c1951829",
   "metadata": {},
   "source": [
    "#final table looks like this:\n",
    "+-------+---------+---------+-------+----+------------------+\n",
    "|station|longitude| latitude|weekday|hour|       criticality|\n",
    "+-------+---------+---------+-------+----+------------------+\n",
    "|      9| 2.185294|41.385006| Friday|   8|0.6129032258064516|\n",
    "|     10| 2.185206|41.384875| Friday|  22| 0.622107969151671|\n",
    "|     58| 2.170736|41.377536| Sunday|  23|0.6239554317548747|\n",
    "|      9| 2.185294|41.385006| Friday|  20|0.6258389261744967|\n",
    "|     58| 2.170736|41.377536| Sunday|  22|0.6323119777158774|\n",
    "+-------+---------+---------+-------+----+------------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "eeaef8d7-a43c-49ba-8ba7-877a953873de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to add the header we create an rdd with the column names\n",
    "#obviously there are different way to do that\n",
    "header = (\"Station\", \"Lattitude\", \"Longtitude\", \"Weekday\", \"Hour\", \"Criticality\")\n",
    "header_rdd = sc.parallelize([header])\n",
    "\n",
    "#first we change df to rdd\n",
    "df_to_rdd = final_DF.rdd\n",
    "\n",
    "#we make a union to mix two rdds\n",
    "result_rdd = header_rdd.union(df_to_rdd)\n",
    "\n",
    "\n",
    "#cast data to string then add \\t as seperator\n",
    "saveable_rdd = result_rdd.map(lambda x: str(x[0])+'\\t'+str(x[1])+'\\t'+str(x[2])+'\\t'+str(x[3])+'\\t'+str(x[4])+'\\t'+str(x[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5648ec3e-0f6f-4b5f-9217-83e91c6fa1c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Station\\tLattitude\\tLongtitude\\tWeekday\\tHour\\tCriticality',\n",
       " '9\\t2.185294\\t41.385006\\tFriday\\t8\\t0.6129032258064516',\n",
       " '10\\t2.185206\\t41.384875\\tFriday\\t22\\t0.622107969151671',\n",
       " '58\\t2.170736\\t41.377536\\tSunday\\t23\\t0.6239554317548747',\n",
       " '9\\t2.185294\\t41.385006\\tFriday\\t20\\t0.6258389261744967',\n",
       " '58\\t2.170736\\t41.377536\\tSunday\\t22\\t0.6323119777158774']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#saving  the final result as csv file\n",
    "saveable_rdd.coalesce(1).saveAsTextFile(\"lab3_s308885_final_result.csv/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e32a6c8a-9b78-4d75-8ba6-9c1c816af9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or we can use this method\n",
    "#final_DF.write.csv(\"final_df_stations_seperated/\", header=True,sep= '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6720b156-8cce-44d1-ac6d-fc58060e10db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Yarn)",
   "language": "python",
   "name": "pyspark_yarn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
